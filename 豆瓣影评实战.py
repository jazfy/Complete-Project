
# coding: utf-8

# # è±†ç“£å½±è¯„åˆ†æå®æˆ˜
        æŠ€æœ¯å±‚é¢å®ç°ä¸­æ–‡æ–‡æœ¬åˆ†æ,ä¸ä»…å¯æ·±åˆ»æŒæ¡äººåŠ›æ— æ³•ä¼åŠçš„æœ¬è´¨å†…å®¹,è€Œä¸”åœ¨å‘ˆç°æ–¹å¼ä¸Šå¸¸ä¸å›¾è¡¨ç­‰æ–¹å¼ç»“åˆ,æˆ‘ä»¬å§‹ç»ˆç›¸ä¿¡,æŠ€æœ¯æ˜¯å…·æœ‰æ™®ä¸–ä»·å€¼çš„,ä¸åº”å±€é™äºè¢«æŸä¸ªç‰¹å®šç¾¤ä½“å„æ–­.
        ä»Šå¤©,ä»Pythonå¼€å§‹,ä»¥è±†ç“£å½±è¯„æ•°æ®ä¸ºå¯¹è±¡,è¿›è¡ŒçœŸæ­£æ„ä¹‰ä¸Šçš„æ–‡æœ¬æŒ–æ˜,goğŸ¶ğŸ¶ğŸ¶!
# ## åˆ†è¯
        ä¸­æ–‡æ–‡æœ¬åœ¨å¤„ç†ä¸Š,å’Œè‹±æ–‡æœ€å¤§çš„ä¸åŒä¹‹å¤„å°±åœ¨äºéœ€è¦å°†æ–‡ç« åˆ‡åˆ†ä¸ºä¸€ç»„ç»„å…·æœ‰ç‰¹å®šæ„ä¹‰çš„è¯æ±‡å’ŒçŸ­è¯­,åˆ†è¯å¹¶éåªæ˜¯ç®€å•æŠŠæ–‡ç« åˆ‡å¼€å³å¯,å®é™…ä¸Šè¿˜å­˜åœ¨è¿‡æ»¤åœç”¨è¯ç­‰æ–¹é¢,è€Œé‡‡ç”¨ä½•ç§åˆ†è¯ç®—æ³•æ˜¯æˆ‘ä»¬ä¸å¿…å…³æ³¨çš„,å› ä¸ºjiebaè¶³å¤Ÿå¥½ç”¨!
# ### åŠ è½½æ•°æ®å’Œå¯¼å…¥ä¾èµ–åº“

# In[1]:


import jieba #ç»“å·´åˆ†è¯
import pandas as pd #pandasåº“,å¤„ç†æ•°æ®å²‚èƒ½ä¸ç”¨,ç®€ç›´å®Œç¾å–ä»£Excel

basic_information = pd.read_csv('/Users/meininghang/Downloads/douban_movie/basic_information.csv') #è¯»å–åŸºæœ¬ä¿¡æ¯æ–‡ä»¶
basic_information.head(10) #æ˜¾ç¤ºå‰åé¡¹


# In[2]:


movie_basic_information = pd.read_csv('/Users/meininghang/Downloads/douban_movie/movie_basic_information.csv') #å¦ä¸€ä»½åŸºæœ¬ä¿¡æ¯æ–‡ä»¶
movie_basic_information.head(10) #æ˜¾ç¤ºå‰åé¡¹


# In[3]:


comment_information = pd.read_csv('/Users/meininghang/Downloads/douban_movie/comments.csv') #åŠ è½½å½±è¯„ä¿¡æ¯
comment_information.head(10) #æ˜¾ç¤ºå‰åé¡¹


# ### æ¸…æ´—æ•°æ®
        æ³¨æ„åˆ°é‚®ä»¶ä¸Šè®¤ä¸ºæŠ“å–çš„æ˜¯å˜3ä¸€éƒ¨ç”µå½±çš„è¯„è®º,å…¶å®å¹¶ä¸æ˜¯ä¸€éƒ¨,è€Œæ˜¯å¾ˆå¤šéƒ¨,ä½†åœ¨å®è·µä¸­,æˆ‘ä»¬åœ¨åˆ†ææ—¶,ä¹Ÿæ˜¯è¦åˆ†é—¨åˆ«ç±»è¿›è¡Œåˆ†æ,æ•°æ®åˆ†æå¯ä»¥å‡è½»ä¸€äº›äººå·¥æ£€ç´¢å’Œæ¢ç´¢ç‰¹å¾çš„å·¥ä½œ,ä½†å…¶ä¹Ÿæœ‰å±€é™æ€§,ç­›é€‰æœ‰æ„ä¹‰çš„æ•°æ®,ç¥›é™¤æ— æ„ä¹‰çš„æ•°æ®å ç”¨äº†å¤§é‡æ—¶é—´,å› æ­¤,ä¸ºè´´åˆå®é™…,æˆ‘ä»¬ç­›é€‰å‡ºå…³äºå˜3çš„å„é¡¹ä¿¡æ¯.
# In[4]:


basic_information.å½±ç‰‡å.unique() #æŸ¥çœ‹å½±ç‰‡æ•°é‡


# In[5]:


basic_information.å½±ç‰‡å.value_counts() #æŸ¥çœ‹å„å½±ç‰‡å…·ä½“æ•°é‡


# In[6]:


T_3 = basic_information.iloc[:40] #å˜3åŸºæœ¬ä¿¡æ¯å…±æœ‰40è¡Œ,å¾€ä¸Šé¢ğŸ‘†,æœ€åä¸€è¡Œ,æ‰€ä»¥æˆªå–å…¨éƒ¨ä¿¡æ¯,æ³¨æ„0æ˜¯ç¬¬ä¸€ä¸ªèµ·å§‹æ•°å­—
#ç”±äºæ­¤æ—¶æ•°æ®é‡å¾ˆå°‘,äººå·¥æŸ¥çœ‹ç›¸å…³ä¿¡æ¯å³å¯,æˆ‘ä»¬å®é™…ä¸Šéœ€è¦çš„ä¿¡æ¯åªæœ‰æ¼”èŒäººå‘˜/å§“åä¸¤åˆ—,å•ç‹¬æ‹¿å‡ºæ¥å³å¯,å…¶ä½™ä¿¡æ¯éšç”¨éšå–


# In[7]:


#é‡æ„ä¿¡æ¯
T_3_information = T_3[['å§“å','æ¼”èŒäººå‘˜']] #é€‰å–ä¿¡æ¯
T_3_information = T_3_information.drop_duplicates(['å§“å']) #å»é‡
T_3_information.index = [1,2,3,4,5,6,7,8,9,10,11,12] #é‡å‘½ååˆ—
T_3_information #æ˜¾ç¤ºä¿¡æ¯,æ­¤æ•°æ®ç”¨äºåˆ†æå„æ¼”å‘˜åœ¨è¯„è®ºä¸­çš„è¡¨ç°


# In[8]:


movie_basic_information.ç”µå½±åç§°.unique() #å¯¹å¦ä¸€ä»½ä¿¡æ¯æ•°æ®è¿›è¡ŒæŸ¥éªŒ,å¹¶æ‰¾åˆ°å˜3çš„ä¿¡æ¯


# In[9]:


T_3_Star = movie_basic_information[movie_basic_information.ç”µå½±åç§° == 'å˜å½¢é‡‘åˆš3 Transformers: Dark of the Moon'] #è®¾å®šç­›é€‰æ ‡å‡†
T_3_Star = T_3_Star.T#è¯„åˆ†ä¿¡æ¯æ¯”è¾ƒç‰¹æ®Š,æ¯éƒ¨ç”µå½±åªæœ‰ä¸€ä¸ªä¿¡æ¯

T_3_Star


# In[10]:


#æ±‡æ€»åŸºæœ¬ä¿¡æ¯
T_3_INF = pd.read_csv('/Users/meininghang/Downloads/douban_movie/T_3_information.csv')
T_3_INF


# In[11]:


T_3_comment = comment_information[comment_information.ç”µå½±åç§° == 'å˜å½¢é‡‘åˆš3çš„å½±è¯„ (1590)'] #ç­›é€‰å½±è¯„ä¿¡æ¯
T_3_comment.head() #æŸ¥çœ‹å‰äº”é¡¹


# In[12]:


T_3_comment.info() #æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯


# #### å¼€å§‹åˆ†è¯

# #####  è¿‡æ»¤åœç”¨è¯
        åœç”¨è¯æ˜¯æŒ‡ç¼ºä¹å®åœ¨æ„ä¹‰çš„è¯å’Œæ ‡ç‚¹ç¬¦å·ç­‰å†…å®¹,å¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œç­›é€‰,å¯ä»¥æœ‰æ•ˆå‡å°‘'å™ªéŸ³'
# In[13]:


#åŠ è½½åœç”¨è¯å…¸,åœ¨æ­¤é€‰ç”¨å“ˆå·¥å¤§åœç”¨è¯å…¸
with open('/Users/meininghang/Downloads/stopwords-master/å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt') as f:
    stop_words = f.read()


# In[14]:


stop_words.splitlines() #ç¥›é™¤æ¢è¡Œç¬¦


# In[15]:


all_title_seg = [] #æ‰€æœ‰é¢˜ç›®åˆ†è¯
for i in T_3_comment.é¢˜ç›®:
    single_title_seg = [] #å­˜æ”¾å•ä¸ªé¢˜ç›®åˆ†è¯ç»“æœ
    segs = jieba.cut(i) #å¯¹æ¯ä¸ªé¢˜ç›®è¿›è¡Œåˆ‡åˆ†
    for seg in segs:
        if seg not in stop_words: #åˆ¤æ–­æ˜¯å¦åœ¨åœç”¨è¯å…¸å†…,
            single_title_seg.append(seg) #å•ä¸ªè¯„è®ºé¢˜ç›®åŠ å…¥[]
    all_title_seg.append(single_title_seg) #å…¨ä½“åŠ å…¥
    


# In[16]:


print('/'.join (all_title_seg[9])) #æŸ¥éªŒ


# In[17]:


#ä¸»è§’,æ‰€æœ‰è¯„è®º,çš†ä¸ä¸Šç±»ä¼¼
all_comment_seg = [] 
for i in T_3_comment.è¯„è®ºå†…å®¹:
    single_comment_seg = []
    segs = jieba.cut(i)
    for seg in segs:
        if seg not in stop_words:
            single_comment_seg.append(seg)
    all_comment_seg.append(single_comment_seg)


# In[18]:


print('/'.join(all_comment_seg[388]))


# ## æƒ…æ„Ÿåˆ†æ
        æƒ…æ„Ÿåˆ†æå®é™…ä¸Šæ˜¯äºŒå…ƒåˆ†ç±»çš„æ¨æ¼”,é€šè¿‡æ¦‚ç‡æ¥åˆ¤æ–­ç»™å®šæ ·æœ¬çš„æ¦‚ç‡å¤§å°,éšåè®¾å®šé˜ˆå€¼,è¶…è¿‡æ­¤é˜ˆå€¼åˆ™ç»™å‡ºåˆ¤æ–­,åœ¨æœ¬æ¡ˆä¾‹ä¸­,æƒ…æ„Ÿåˆ†æå®é™…ä¸Šæœ‰æŸç§é¢„ç¤º,äº”æ˜Ÿå¥½è¯„å’Œä¸€æ˜Ÿå·®è¯„çš„æƒ…æ„Ÿå€¾å‘ä¸€å®šæ˜¯ä¸åŒçš„,è¿™ä¹Ÿæ˜¯è¿›è¡Œæƒ…æ„Ÿåˆ¤æ–­çš„å‡ºå‘ç‚¹.
# In[21]:


from snownlp import SnowNLP #åŠ è½½snownlp


# In[22]:


#test
s = SnowNLP(T_3_comment.é¢˜ç›®[1])


# In[23]:


s.words #è¯


# In[26]:


s.tags #è¯æ€§æ ‡æ³¨


# In[27]:


for i in s.tags:
    print(i)


# In[31]:


s.sentiments #æƒ…ç»ª


# In[32]:


s.keywords(3) #å…³é”®è¯


# In[37]:


s.summary(3)  #æ‘˜è¦,ä¼¼ä¹æ²¡æœ‰æ•ˆæœ,å¯èƒ½å› ä¸ºå¤ªçŸ­


# In[39]:


s.sentences #åˆ‡å¥å­


# In[42]:


s.tf #tf


# In[43]:


s.idf #idf


# In[46]:


s.sim(['ä½œ']) #ç›¸ä¼¼åº¦


# In[47]:


s = SnowNLP([[T_3_comment.é¢˜ç›®[1],T_3_comment.é¢˜ç›®[0],T_3_comment.é¢˜ç›®[7]]])


# In[66]:


from snownlp import SnowNLP
s = SnowNLP(T_3_comment.è¯„è®ºå†…å®¹[1])
print(s.summary(3),) #ç”¨æ®µè½æ¯”è¾ƒå¥½

å¯¹snownlpçš„è®¤è¯†ç»“æŸ,å¼€å§‹å®è·µ
# ### æƒ…æ„Ÿåˆ†æ

# In[74]:


from snownlp.sentiment import Sentiment
import matplotlib.pyplot as plt #ç”»å›¾è¡¨ç¤ºä¸‹
import numpy as np


# In[75]:


def s_nlp_sentiment(self):
    sentiment_fruit = []
    for i in self:
        s_t = SnowNLP(i)
        sentiment_fruit.append(s_t.sentiments)
    plt.hist(sentiment_fruit,bins=np.arange(0,1,0.01))
    plt.show()


# In[81]:


sent = []
for i in T_3_comment.é¢˜ç›®:
   #print(i)
   s = SnowNLP(i)
   senti = s.sentiments
   sent.append(senti)
   #plt.hist(sent,bins=np.arange(0,1,0.01))
   #plt.show()


# In[89]:


len(sent)#æ£€æŸ¥


# In[91]:


T_3_comment['æƒ…ç»ª'] = sent


# In[95]:


T_3_comment.to_csv('/Users/meininghang/Desktop/T_3_comment.csv') #å¯¼å‡º


# In[98]:


#åŠ è½½æ˜Ÿçº§å’Œæƒ…ç»ªå¾—åˆ†ä¹‹é—´çš„å…³ç³»

        ä»å…¶ä¸­å¯å¾—å‡ºä¸¤ç§ç»“è®º:
        1.è¯„åˆ†äººæ•´ä½“ä¸Šå¯¹ç”µå½±æŒè®¤åŒæ€åº¦,æƒ…ç»ªå¹³å‡å€¼é«˜äº0.5;
        2.æ˜Ÿçº§å’Œæƒ…ç»ªæŒæ­£ç›¸å…³,å³è¯„åˆ†è¶Šé«˜,å°è±¡ä¹Ÿè¾ƒä¸ºæ­£é¢.
# ## ä¸»é¢˜å’Œæ–‡æœ¬ç›¸ä¼¼åº¦

# In[1]:


import pandas as pd
import jieba
import gensim #æ„å»ºä¾èµ–åº“
from gensim import corpora


# In[2]:


#åŠ è½½æ•°æ®
T_3_comment = pd.read_csv('/Users/meininghang/douban-movie-sets-anyalase/T_3_comment.csv')
T_3_comment['è¯„è®ºå†…å®¹'][:5] #æŸ¥çœ‹å†…å®¹


# In[6]:


with open("/Users/meininghang/Downloads/stopwords-master/å››å·å¤§å­¦æœºå™¨æ™ºèƒ½å®éªŒå®¤åœç”¨è¯åº“.txt") as f: #åœç”¨è¯
    stop_words = f.read().splitlines()


# In[8]:


comment = []
for i in T_3_comment.è¯„è®ºå†…å®¹:
    comment_seg = [] #åŠ è½½å•ä¸ªè¯„è®ºåˆ†è¯ç»“æœ
    segs = jieba.cut(i) #åˆ†è¯
    for seg in segs:
        if seg not in stop_words:# è¿‡æ»¤
            comment_seg.append(seg)
    comment.append(comment_seg)


# In[11]:


comment[0] #æŸ¥éªŒ


# In[12]:


#TF-IDF
dictionary = corpora.Dictionary(comment)


# In[13]:


cipin = [dictionary.doc2bow(comm) for comm in comment]
cipin


# In[18]:


from gensim import models
from gensim.corpora import corpus
tf_idf_model = models.TfidfModel(cipin)
tfidf = tf_idf_model[cipin]
#tf_idf_mat = #corpus2dense(tfidf,len(dictionary))


# In[19]:


model = gensim.models.Word2Vec(comment,size=1000,window=5,min_count=5)


# In[20]:


model['è¯„è®º']


# In[36]:


model.similarity('å‰§','ç”µå½±')


# In[ ]:


#jeibatf-idfç¤ºä¾‹
'''åŸºäº TF-IDF ç®—æ³•çš„å…³é”®è¯æŠ½å–
import jieba.analyse

jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())
sentence ä¸ºå¾…æå–çš„æ–‡æœ¬
topK ä¸ºè¿”å›å‡ ä¸ª TF/IDF æƒé‡æœ€å¤§çš„å…³é”®è¯ï¼Œé»˜è®¤å€¼ä¸º 20
withWeight ä¸ºæ˜¯å¦ä¸€å¹¶è¿”å›å…³é”®è¯æƒé‡å€¼ï¼Œé»˜è®¤å€¼ä¸º False
allowPOS ä»…åŒ…æ‹¬æŒ‡å®šè¯æ€§çš„è¯ï¼Œé»˜è®¤å€¼ä¸ºç©ºï¼Œå³ä¸ç­›é€‰
jieba.analyse.TFIDF(idf_path=None) æ–°å»º TFIDF å®ä¾‹ï¼Œidf_path ä¸º IDF é¢‘ç‡æ–‡ä»¶'''


# In[23]:


import jieba
import jieba.analyse


# In[88]:


k = jieba.analyse.extract_tags(T_3_comment.è¯„è®ºå†…å®¹[1],topK=2,withWeight=True)


# In[89]:


k


# In[41]:


k[1][0] #type==str


# In[42]:


model.similarity(k[0][0],k[1][0])


# In[48]:


for i in T_3_comment.è¯„è®ºå†…å®¹:
    k = jieba.analyse.extract_tags(i,topK=2,)


# In[52]:


k


# In[60]:


from gensim.similarities import MatrixSimilarity


# In[62]:


sim_index = MatrixSimilarity(comment)


# In[63]:


sim_index[comment[0]]


# In[64]:


sort_sims = sorted(enumerate(sim_index[comment[0]]), key=lambda item: item[1],reverse=True)  # åˆ©ç”¨ sorted å‡½æ•°æŒ‰ç…§ç›¸ä¼¼åº¦ä»å¤§åˆ°å°æ’åº
print(sort_sims[0:10])  # è¾“å‡ºæœ€ä¸ºç›¸ä¼¼çš„å‰ 10 ä¸ªæ–‡æ¡£ç¼–å·åŠç›¸ä¼¼åº¦


# In[68]:


for j in [i[0] for i in sort_sims[0:10]]:
    print(j,"\n",T_3_comment.è¯„è®ºå†…å®¹[j])   #è¾“å‡ºåŸå§‹æ–‡æ¡£å†…å®¹ï¼ŒæŸ¥çœ‹æ£€ç´¢æ•ˆæœ

        é¦–å…ˆå›æº¯ç¬¬ä¸€ç¯‡å½±è¯„çš„ç›´è§‚æ„Ÿå—æ˜¯"è®½åˆº',å…¶æƒ…ç»ªå¾—åˆ†è¾ƒä½,è€Œæ­¤æ—¶è¿”å›çš„ç›¸è¿‘æ–‡æœ¬å…·æœ‰ç›¸åŒçš„æ„Ÿå—,å³"ä¸æ»¡'
# In[70]:


T_3_comment.æƒ…ç»ª[0]


# ## å¯è§†åŒ–

# In[97]:


get_ipython().system('pip3 install pyldavis')


# In[98]:


import pyLDAvis
import pyLDAvis.gensim
from gensim import corpora,models
from gensim.models.ldamodel import LdaModel


# In[106]:


with open('/Users/meininghang/Downloads/stopwords-master/å››å·å¤§å­¦æœºå™¨æ™ºèƒ½å®éªŒå®¤åœç”¨è¯åº“.txt') as f:
    stop_words = f.read().splitlines()
    comment = []
    for i in T_3_comment.è¯„è®ºå†…å®¹:
        segment = []
        segs = jieba.cut(i)
        for seg in segs:
            if seg not in stop_words:
                segment.append(seg)
        comment.append(segment)


# In[107]:


dictionary = corpora.Dictionary(comment)
corpus = [dictionary.doc2bow(commen) for commen in comment]


# In[108]:


tfidf_model = models.TfidfModel(corpus)
corpus_tfidf = tfidf_model[corpus]


# In[109]:


lda = LdaModel(corpus=corpus_tfidf,id2word=dictionary,num_topics=3)


# In[118]:


lda.show_topics(9)


# In[120]:


vis_data = pyLDAvis.gensim.prepare(lda,corpus,dictionary)
vis_data


# In[123]:


pyLDAvis.display(vis_data)


# In[140]:


k = T_3_comment.è¯„è®ºæ—¶é—´
k = str(k)
print(k,'\n')


# In[146]:


year = []
month = []
day = []

for i in T_3_comment.è¯„è®ºæ—¶é—´:
    year.append(i.split('-')[0])
    month.append(i.split('-')[1])
    day.append(i.split('-')[2])

T_3_comment['year'] = year
T_3_comment['month'] = month
T_3_comment['day'] = day

T_3_comment['count'] = len(T_3_comment) * [1]
T_3_comment.head()


# In[147]:


dataset_ym = pd.pivot_table(T_3_comment,values='count',
                            index = 'year',columns='month',aggfunc='sum')
dataset_ym


# In[148]:


get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sea
sea.heatmap(dataset_ym,linewidths=.5)

        è¡¨ç¤ºä¸åŒå¹´ä»½å„æœˆä»½ç´¯è®¡è¯„è®ºæ•°çƒ­å›¾
# In[149]:


sea.heatmap(dataset_ym,annot=True,fmt="n",linewidths=.5)


# In[151]:


title[2]


# In[155]:


T_3_comment['words'] = comment


# In[156]:


T_3_comment['word_count'] = [i.count('è¯„è®º') for i in T_3_comment['words']]


# In[157]:


dataset_md_words=pd.pivot_table(T_3_comment, values = 'word_count', index = 'month', columns = 'day',aggfunc='sum')


# In[158]:


dataset_md_words


# In[159]:


sea.heatmap(dataset_md_words,linewidths=.5)


# In[160]:


from datetime import datetime


# In[181]:


T_3_comment.è¯„è®ºæ—¶é—´[1]


# In[183]:


time = datetime.strptime(T_3_comment['è¯„è®ºæ—¶é—´'][0], '%Y-%m-%d %H:%M:%S')


# In[185]:


print(time)


# In[192]:


T_3_comment['å…·ä½“æ—¶é—´'] = [datetime.strptime(i,'%Y-%m-%d %H:%M:%S') for i in T_3_comment['è¯„è®ºæ—¶é—´']]


# In[188]:


T_3_comment.å…·ä½“æ—¶é—´.head()


# In[193]:


T_3_comment.å…·ä½“æ—¶é—´.value_counts()


# In[194]:


T_3_comment.è¯„è®ºæ—¶é—´.value_counts().plot(kind='line', rot=0, figsize=(14, 8))


# In[197]:


group_word = T_3_comment.groupby(['è¯„è®ºæ—¶é—´'])['word_count'].sum()


# In[198]:


group_word.plot(kind='line',figsize=(14, 8))

